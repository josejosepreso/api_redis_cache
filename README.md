#  Creaci贸n de un Pipeline de Datos y API con Cach茅 Inteligente

Este proyecto demuestra c贸mo construir una soluci贸n completa de backend capaz de migrar datos a gran escala, exponerlos mediante una API segura y optimizada, y desplegarla en la nube con una estrategia de cach茅 inteligente.

##  Caracter铆sticas Principales

- **Pipeline ETL con Azure Data Factory**: Migraci贸n y transformaci贸n de un dataset desde Azure Blob Storage hacia una base de datos en Azure.
- **API REST con FastAPI**: Backend implementado en Python con endpoints para consulta y gesti贸n de datos.
- **Autenticaci贸n con Firebase**: Protecci贸n de endpoints mediante JWT con roles y permisos definidos (usuarios activos y administradores).
- **Cach茅 con Redis**: Implementaci贸n de caching din谩mico para consultas con invalidaci贸n autom谩tica.
- **Monitoreo con Azure Application Insights**: An谩lisis de desempe帽o, trazas y m茅tricas de la API bajo carga.
- **Despliegue en la nube con Docker**: Contenedor desplegado usando Azure App Service o Azure Container Apps.

##  Endpoints Principales

- `POST /signup`: Registro de nuevos usuarios.
- `POST /login`: Autenticaci贸n y emisi贸n de JWT.
- `GET /catalog`: Consulta de todos los registros del modelo de datos (cacheable)
- `POST /catalog`: Creaci贸n de nuevos registros con l贸gica de invalidaci贸n de cach茅.
- `GET/catalong?category=A`: Consulta de datos con filtros (cacheable)

##  Tecnolog铆as Usadas

- Python + FastAPI
- Azure Data Factory, Blob Storage, SQL
- Firebase Authentication
- Redis
- Azure Application Insights
- Docker + Azure App Service

## 讹 Ejecuci贸n Local
1. Clona el repositorio
2. Crear un archivo `.env` con las variables de entorno necesarias
3. Crear un ambiente ambiente de python `python -m venv env`
4. Habilitar el ambiente virtual `source ./env/bin/activate`
5. Instalar dependencias: `pip install -r requirements.txt`
6. Ejecuta el servidor: `main main.py`
